<!DOCTYPE html>
<html><meta charset="utf-8"><meta content="width=device-width, initial-scale=1" name="viewport"><link href="/index.css" rel="stylesheet"><meta content="summary" name="twitter:card"><meta content="@meekaale" name="twitter:site"><meta content="https://goula.sh/logo-square.png" name="twitter:image"><meta content="[[articles]] Leftovers From Last Time" name="twitter:title"><meta content="Highlights from 📰 Leftovers From Last Time" name="twitter:description"><title>📰 Leftovers From Last Time - goula.sh</title><header><a href="/">goula.sh</a> • <a href="/articles/">Articles</a></header><article class="page"><section><h1><a class="ref" href="/articles">📰</a> Leftovers From Last Time</h1><p id="nnE44ZMZZ"><strong>Author</strong>: <a class="ref" href="/topics/everythingstudies-dot-com">everythingstudies.com</a></p><p id="VgQ1HAiKZ"><strong>Full Title</strong>: Leftovers From Last Time</p><p id="aT5sVVwcM"><strong>URL</strong>: <a href="https://everythingstudies.com/2020/05/22/leftovers-from-last-time/" target="_blank">https://everythingstudies.com/2020/05/22/leftovers-from-last-time/</a></p><h3 id="AQ2lxxRZN">Highlights from <a class="ref" href="/days/2021-03-02">March 2nd, 2021</a>.</h3><div class="indent"><blockquote id="NF3gySOtr">So in order to explain AI, he found he had to explain thought itself, and why human thought wasn’t particularly representative of good thought. So he found he had to explain human thought–all its biases and systematic errors, all its self-delusions and predictable mistakes; he’d found a natural home on Overcoming Bias. And to explain human thought, he found he had to explain–everything, really. It was like when you pull on a loose thread and end up unravelling your entire favourite jumper.</blockquote><blockquote id="jzCYpZFg2">I consider the political left and right wings to be on roughly equal footing, morally and intellectually. They’re both partial narratives that each capture different aspects of the physical and moral universes. Together they form something better than apart, and any cultural milieu that one of them dominates too forcefully invariably deteriorates, like a muscle that faces no resistance.</blockquote><blockquote id="yIEKPNGVD">Most of us fear trusting our abstract reasoning when it takes us to strange places, and that’s often a wise impulse: hubris is a staple of literature for a good reason. Trusting your own thinking over tradition has probably been a bad idea throughout history when the knowledge available to reason from has been patchy and mistakes frequently lethal. I probably have less of this self-distrust than most people do, but Yudkowsky appears to have little to none.</blockquote><blockquote id="aOUYxjQtv">If a line of moral reasoning suggested that I must dedicate my life to tasting every flavor of ice cream in the world, that would probably make it easier for me to accept than if it suggested that I should work around the clock at a job I hate and donate all my earnings to charity. According to Yudkowsky’s reasoning, he and many who agree with him have important and interesting things to do in saving the world.</blockquote><blockquote id="_Gk7tnRMc">Perhaps it even explains the weird-to-immoral slide: people like this are unpredictable and cannot necessarily be trusted to follow convention and do what is expected of them, which easily translates to “cannot be trusted” in general.</blockquote></div></section></article></html>